{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Churn Prediction Model\n",
    "\n",
    "This notebook implements a churn prediction model for a music streaming service.\n",
    "\n",
    "## Day 1: Data Pipeline & Feature Engineering Foundation\n",
    "\n",
    "**Goal**: Transform event logs into user-level features for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step-1-1-header",
   "metadata": {},
   "source": [
    "## Step 1.1: Load Data and Create Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Train: 17,499,636 rows\n",
      "Test: 4,393,179 rows\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "train = pd.read_parquet('train.parquet')\n",
    "test = pd.read_parquet('test.parquet')\n",
    "\n",
    "print(f\"Train: {train.shape[0]:,} rows\")\n",
    "print(f\"Test: {test.shape[0]:,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "create-labels",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total users: 19,140\n",
      "Churned users: 4,271\n",
      "Churn rate: 22.31%\n",
      "\n",
      "Churn times recorded for 4,271 users\n"
     ]
    }
   ],
   "source": [
    "# Create user-level churn labels\n",
    "churned_users = train[train['page'] == 'Cancellation Confirmation']['userId'].unique()\n",
    "all_users = train['userId'].unique()\n",
    "\n",
    "print(f\"Total users: {len(all_users):,}\")\n",
    "print(f\"Churned users: {len(churned_users):,}\")\n",
    "print(f\"Churn rate: {len(churned_users)/len(all_users):.2%}\")\n",
    "\n",
    "# Get churn timestamps for temporal slicing later\n",
    "churn_times = train[train['page'] == 'Cancellation Confirmation'].groupby('userId')['time'].first()\n",
    "print(f\"\\nChurn times recorded for {len(churn_times):,} users\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step-1-2-header",
   "metadata": {},
   "source": [
    "## Step 1.2: Vectorized Feature Engineering\n",
    "\n",
    "Using `groupby` and vectorized pandas operations for ~100x speedup over row-by-row processing.\n",
    "\n",
    "Feature categories:\n",
    "- **Engagement**: total events, songs, sessions\n",
    "- **Behavioral ratios**: thumbs up/down per song, error rate\n",
    "- **Temporal**: days active, recency, activity trend\n",
    "- **Subscription**: paid/free status, level changes, downgrade/upgrade events\n",
    "- **Content diversity**: unique songs/artists, listen time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "feature-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_features_vectorized(df, churn_times_series=None):\n",
    "    \"\"\"\n",
    "    Vectorized feature engineering using groupby operations.\n",
    "    ~100x faster than row-by-row processing.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame with events\n",
    "    - churn_times_series: Series with userId as index, churn time as value (for Cancel page handling)\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with one row per user\n",
    "    \"\"\"\n",
    "    print(\"Computing basic aggregations...\")\n",
    "\n",
    "    # ===== BASIC AGGREGATIONS =====\n",
    "    basic_agg = df.groupby('userId').agg(\n",
    "        total_events=('page', 'count'),\n",
    "        total_sessions=('sessionId', 'nunique'),\n",
    "        time_min=('time', 'min'),\n",
    "        time_max=('time', 'max'),\n",
    "        registration=('registration', 'first'),\n",
    "        gender=('gender', 'first'),\n",
    "        location=('location', 'first'),\n",
    "        level_last=('level', 'last'),\n",
    "    )\n",
    "\n",
    "    # ===== PAGE COUNTS (using crosstab for efficiency) =====\n",
    "    print(\"Computing page counts...\")\n",
    "    important_pages = ['NextSong', 'Thumbs Up', 'Thumbs Down', 'Add to Playlist',\n",
    "                       'Add Friend', 'Downgrade', 'Upgrade', 'Error', 'Help',\n",
    "                       'Home', 'Settings', 'Roll Advert', 'Logout']\n",
    "\n",
    "    # Filter to important pages only, then crosstab\n",
    "    page_df = df[df['page'].isin(important_pages)][['userId', 'page']]\n",
    "    page_counts = pd.crosstab(page_df['userId'], page_df['page'])\n",
    "\n",
    "    # Ensure all important pages exist as columns\n",
    "    for page in important_pages:\n",
    "        if page not in page_counts.columns:\n",
    "            page_counts[page] = 0\n",
    "\n",
    "    # Rename columns\n",
    "    page_counts.columns = [f'page_{col.lower().replace(\" \", \"_\")}' for col in page_counts.columns]\n",
    "\n",
    "    # ===== SESSION STATISTICS =====\n",
    "    print(\"Computing session statistics...\")\n",
    "    session_sizes = df.groupby(['userId', 'sessionId']).size().reset_index(name='session_length')\n",
    "    session_stats = session_sizes.groupby('userId')['session_length'].agg(\n",
    "        avg_session_length='mean',\n",
    "        max_session_length='max',\n",
    "        std_session_length='std'\n",
    "    ).fillna(0)\n",
    "\n",
    "    # ===== SONG-RELATED FEATURES =====\n",
    "    print(\"Computing song features...\")\n",
    "    songs_df = df[df['page'] == 'NextSong']\n",
    "\n",
    "    song_agg = songs_df.groupby('userId').agg(\n",
    "        total_songs=('page', 'count'),\n",
    "        unique_songs=('song', 'nunique'),\n",
    "        unique_artists=('artist', 'nunique'),\n",
    "        avg_song_length=('length', lambda x: x.clip(upper=1200).mean()),\n",
    "        total_listen_time=('length', lambda x: x.clip(upper=1200).sum()),\n",
    "        std_song_length=('length', lambda x: x.clip(upper=1200).std()),\n",
    "    ).fillna(0)\n",
    "\n",
    "    # ===== LEVEL CHANGES =====\n",
    "    print(\"Computing subscription features...\")\n",
    "    df_sorted = df.sort_values(['userId', 'time'])\n",
    "    df_sorted['level_changed'] = (df_sorted['level'] != df_sorted.groupby('userId')['level'].shift()).astype(int)\n",
    "    level_changes = df_sorted.groupby('userId')['level_changed'].sum() - 1  # subtract 1 for first row\n",
    "    level_changes = level_changes.clip(lower=0)\n",
    "\n",
    "    # Paid events ratio\n",
    "    df_sorted['is_paid_event'] = (df_sorted['level'] == 'paid').astype(int)\n",
    "    paid_ratio = df_sorted.groupby('userId')['is_paid_event'].mean()\n",
    "\n",
    "    # ===== ACTIVITY TREND =====\n",
    "    print(\"Computing temporal features...\")\n",
    "    def compute_activity_trend(group):\n",
    "        if len(group) <= 1:\n",
    "            return 0\n",
    "        mid_time = group['time'].min() + (group['time'].max() - group['time'].min()) / 2\n",
    "        first_half = (group['time'] <= mid_time).sum()\n",
    "        second_half = (group['time'] > mid_time).sum()\n",
    "        return (second_half - first_half) / max(first_half, 1)\n",
    "\n",
    "    activity_trend = df.groupby('userId').apply(compute_activity_trend, include_groups=False)\n",
    "\n",
    "    # ===== CANCEL PAGE VISITS (with 12-hour exclusion) =====\n",
    "    print(\"Computing cancel page visits...\")\n",
    "    cancel_df = df[df['page'] == 'Cancel'][['userId', 'time']].copy()\n",
    "\n",
    "    if churn_times_series is not None and len(cancel_df) > 0:\n",
    "        # Merge churn times\n",
    "        cancel_df = cancel_df.merge(\n",
    "            churn_times_series.rename('churn_time').reset_index(),\n",
    "            on='userId',\n",
    "            how='left'\n",
    "        )\n",
    "        # Count only cancels > 12 hours before churn (or all if no churn)\n",
    "        cancel_df['is_safe'] = (\n",
    "            cancel_df['churn_time'].isna() |\n",
    "            (cancel_df['time'] < cancel_df['churn_time'] - pd.Timedelta(hours=12))\n",
    "        )\n",
    "        cancel_page_visits = cancel_df[cancel_df['is_safe']].groupby('userId').size()\n",
    "    else:\n",
    "        cancel_page_visits = cancel_df.groupby('userId').size()\n",
    "\n",
    "    # ===== COMBINE ALL FEATURES =====\n",
    "    print(\"Combining features...\")\n",
    "    features = basic_agg.copy()\n",
    "\n",
    "    # Join page counts\n",
    "    features = features.join(page_counts, how='left').fillna(0)\n",
    "\n",
    "    # Join session stats\n",
    "    features = features.join(session_stats, how='left').fillna(0)\n",
    "\n",
    "    # Join song features\n",
    "    features = features.join(song_agg, how='left').fillna(0)\n",
    "\n",
    "    # Add level changes and paid ratio\n",
    "    features['level_changes'] = level_changes\n",
    "    features['paid_ratio'] = paid_ratio\n",
    "\n",
    "    # Add activity trend\n",
    "    features['activity_trend'] = activity_trend\n",
    "\n",
    "    # Add cancel page visits\n",
    "    features['cancel_page_visits'] = cancel_page_visits.reindex(features.index).fillna(0).astype(int)\n",
    "\n",
    "    # ===== COMPUTE DERIVED FEATURES =====\n",
    "    print(\"Computing derived features...\")\n",
    "\n",
    "    # Temporal features\n",
    "    features['days_active'] = (features['time_max'] - features['time_min']).dt.days + 1\n",
    "    features['days_since_registration'] = (features['time_max'] - features['registration']).dt.days\n",
    "    features['events_per_day'] = features['total_events'] / features['days_active'].clip(lower=1)\n",
    "    features['songs_per_day'] = features['total_songs'] / features['days_active'].clip(lower=1)\n",
    "\n",
    "    # Subscription features\n",
    "    features['is_paid'] = (features['level_last'] == 'paid').astype(int)\n",
    "    features['has_downgrade'] = (features['page_downgrade'] > 0).astype(int)\n",
    "    features['has_upgrade'] = (features['page_upgrade'] > 0).astype(int)\n",
    "\n",
    "    # Behavioral ratios\n",
    "    features['thumbs_up_ratio'] = features['page_thumbs_up'] / features['total_songs'].clip(lower=1)\n",
    "    features['thumbs_down_ratio'] = features['page_thumbs_down'] / features['total_songs'].clip(lower=1)\n",
    "    features['playlist_add_ratio'] = features['page_add_to_playlist'] / features['total_songs'].clip(lower=1)\n",
    "    features['error_rate'] = features['page_error'] / features['total_events'].clip(lower=1)\n",
    "    features['ad_ratio'] = features['page_roll_advert'] / features['total_songs'].clip(lower=1)\n",
    "\n",
    "    # Song repeat ratio\n",
    "    features['song_repeat_ratio'] = features['total_songs'] / features['unique_songs'].clip(lower=1)\n",
    "\n",
    "    # Fix ratios for users with 0 songs\n",
    "    zero_songs = features['total_songs'] == 0\n",
    "    ratio_cols = ['thumbs_up_ratio', 'thumbs_down_ratio', 'playlist_add_ratio', 'ad_ratio', 'song_repeat_ratio']\n",
    "    features.loc[zero_songs, ratio_cols] = 0\n",
    "\n",
    "    # ===== EXTRACT STATE FROM LOCATION =====\n",
    "    def extract_state(loc):\n",
    "        if pd.isna(loc) or loc == 'Unknown':\n",
    "            return 'Unknown'\n",
    "        if ',' in str(loc):\n",
    "            return str(loc).split(',')[-1].strip()[:2]\n",
    "        return 'Unknown'\n",
    "\n",
    "    features['state'] = features['location'].apply(extract_state)\n",
    "\n",
    "    # Fix gender - ensure it's always a string (handles mixed types from NaN)\n",
    "    features['gender'] = features['gender'].fillna('Unknown').astype(str)\n",
    "\n",
    "    # ===== CLEANUP =====\n",
    "    # Drop intermediate columns\n",
    "    features = features.drop(columns=['time_min', 'time_max', 'registration', 'location', 'level_last'])\n",
    "\n",
    "    # Reset index to make userId a column\n",
    "    features = features.reset_index()\n",
    "\n",
    "    print(f\"Done! Created {len(features)} user feature rows with {len(features.columns)-1} features\")\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step-1-3-header",
   "metadata": {},
   "source": [
    "## Step 1.3: Create Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "create-train-features",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating features for training users...\n",
      "Processing 19,140 users from 17,499,636 events\n",
      "\n",
      "Computing basic aggregations...\n",
      "Computing page counts...\n",
      "Computing session statistics...\n",
      "Computing song features...\n",
      "Computing subscription features...\n",
      "Computing temporal features...\n",
      "Computing cancel page visits...\n",
      "Combining features...\n",
      "Computing derived features...\n",
      "Done! Created 19140 user feature rows with 43 features\n",
      "\n",
      "Completed in 34.4 seconds\n",
      "\n",
      "Training set shape: (19140, 45)\n",
      "Churn distribution:\n",
      "churn\n",
      "0    14869\n",
      "1     4271\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create features for all training users using vectorized operations\n",
    "print(\"Creating features for training users...\")\n",
    "print(f\"Processing {len(all_users):,} users from {len(train):,} events\\n\")\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "train_features = create_user_features_vectorized(train, churn_times_series=churn_times)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\nCompleted in {elapsed:.1f} seconds\")\n",
    "\n",
    "# Add churn labels\n",
    "churned_set = set(churned_users)\n",
    "train_features['churn'] = train_features['userId'].apply(lambda x: 1 if x in churned_set else 0)\n",
    "\n",
    "print(f\"\\nTraining set shape: {train_features.shape}\")\n",
    "print(f\"Churn distribution:\\n{train_features['churn'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "verify-train-features",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature columns:\n",
      "['userId', 'total_events', 'total_sessions', 'gender', 'page_add_friend', 'page_add_to_playlist', 'page_downgrade', 'page_error', 'page_help', 'page_home', 'page_logout', 'page_nextsong', 'page_roll_advert', 'page_settings', 'page_thumbs_down', 'page_thumbs_up', 'page_upgrade', 'avg_session_length', 'max_session_length', 'std_session_length', 'total_songs', 'unique_songs', 'unique_artists', 'avg_song_length', 'total_listen_time', 'std_song_length', 'level_changes', 'paid_ratio', 'activity_trend', 'cancel_page_visits', 'days_active', 'days_since_registration', 'events_per_day', 'songs_per_day', 'is_paid', 'has_downgrade', 'has_upgrade', 'thumbs_up_ratio', 'thumbs_down_ratio', 'playlist_add_ratio', 'error_rate', 'ad_ratio', 'song_repeat_ratio', 'state', 'churn']\n",
      "\n",
      "Total features: 43\n",
      "\n",
      "No missing values in features!\n"
     ]
    }
   ],
   "source": [
    "# Verify feature quality\n",
    "print(\"Feature columns:\")\n",
    "print(train_features.columns.tolist())\n",
    "print(f\"\\nTotal features: {len(train_features.columns) - 2}\")  # excluding userId and churn\n",
    "\n",
    "# Check for any missing values\n",
    "missing = train_features.isnull().sum()\n",
    "if missing.sum() > 0:\n",
    "    print(\"\\nMissing values:\")\n",
    "    print(missing[missing > 0])\n",
    "else:\n",
    "    print(\"\\nNo missing values in features!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "train-feature-stats",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key feature statistics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_events</th>\n",
       "      <th>total_songs</th>\n",
       "      <th>total_sessions</th>\n",
       "      <th>days_active</th>\n",
       "      <th>thumbs_down_ratio</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>has_downgrade</th>\n",
       "      <th>activity_trend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>19140.000000</td>\n",
       "      <td>19140.000000</td>\n",
       "      <td>19140.000000</td>\n",
       "      <td>19140.000000</td>\n",
       "      <td>19140.000000</td>\n",
       "      <td>19140.000000</td>\n",
       "      <td>19140.000000</td>\n",
       "      <td>19140.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>914.296552</td>\n",
       "      <td>746.678840</td>\n",
       "      <td>10.885998</td>\n",
       "      <td>32.147910</td>\n",
       "      <td>0.013100</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.636677</td>\n",
       "      <td>2.149112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1079.652218</td>\n",
       "      <td>898.682491</td>\n",
       "      <td>10.654959</td>\n",
       "      <td>15.689274</td>\n",
       "      <td>0.014867</td>\n",
       "      <td>0.002589</td>\n",
       "      <td>0.480969</td>\n",
       "      <td>16.869290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.998270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>202.000000</td>\n",
       "      <td>155.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.006939</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.370469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>537.500000</td>\n",
       "      <td>428.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>0.010383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1213.000000</td>\n",
       "      <td>991.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>0.015185</td>\n",
       "      <td>0.001351</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.136364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10998.000000</td>\n",
       "      <td>9248.000000</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>934.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       total_events   total_songs  total_sessions   days_active  \\\n",
       "count  19140.000000  19140.000000    19140.000000  19140.000000   \n",
       "mean     914.296552    746.678840       10.885998     32.147910   \n",
       "std     1079.652218    898.682491       10.654959     15.689274   \n",
       "min        1.000000      0.000000        1.000000      1.000000   \n",
       "25%      202.000000    155.000000        4.000000     21.000000   \n",
       "50%      537.500000    428.000000        8.000000     37.000000   \n",
       "75%     1213.000000    991.000000       14.000000     45.000000   \n",
       "max    10998.000000   9248.000000      116.000000     50.000000   \n",
       "\n",
       "       thumbs_down_ratio    error_rate  has_downgrade  activity_trend  \n",
       "count       19140.000000  19140.000000   19140.000000    19140.000000  \n",
       "mean            0.013100      0.001019       0.636677        2.149112  \n",
       "std             0.014867      0.002589       0.480969       16.869290  \n",
       "min             0.000000      0.000000       0.000000       -0.998270  \n",
       "25%             0.006939      0.000000       0.000000       -0.370469  \n",
       "50%             0.010383      0.000000       1.000000        0.111111  \n",
       "75%             0.015185      0.001351       1.000000        1.136364  \n",
       "max             0.500000      0.111111       1.000000      934.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic statistics for key features\n",
    "key_features = ['total_events', 'total_songs', 'total_sessions', 'days_active',\n",
    "                'thumbs_down_ratio', 'error_rate', 'has_downgrade', 'activity_trend']\n",
    "\n",
    "print(\"Key feature statistics:\")\n",
    "train_features[key_features].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "save-train-features",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features saved to train_features.parquet\n"
     ]
    }
   ],
   "source": [
    "# Save training features\n",
    "train_features.to_parquet('train_features.parquet', index=False)\n",
    "print(\"Training features saved to train_features.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step-1-4-header",
   "metadata": {},
   "source": [
    "## Step 1.4: Create Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "create-test-features",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating features for 2,904 test users from 4,393,179 events\n",
      "\n",
      "Computing basic aggregations...\n",
      "Computing page counts...\n",
      "Computing session statistics...\n",
      "Computing song features...\n",
      "Computing subscription features...\n",
      "Computing temporal features...\n",
      "Computing cancel page visits...\n",
      "Combining features...\n",
      "Computing derived features...\n",
      "Done! Created 2904 user feature rows with 43 features\n",
      "\n",
      "Completed in 6.3 seconds\n",
      "Test set shape: (2904, 44)\n"
     ]
    }
   ],
   "source": [
    "# Create features for test users using vectorized operations\n",
    "test_users = test['userId'].unique()\n",
    "print(f\"Creating features for {len(test_users):,} test users from {len(test):,} events\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "test_features = create_user_features_vectorized(test, churn_times_series=None)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\nCompleted in {elapsed:.1f} seconds\")\n",
    "print(f\"Test set shape: {test_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "verify-test-features",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature columns match between train and test!\n",
      "\n",
      "No missing values in test features!\n"
     ]
    }
   ],
   "source": [
    "# Verify test features match train features (excluding churn column)\n",
    "train_cols = set(train_features.columns) - {'churn'}\n",
    "test_cols = set(test_features.columns)\n",
    "\n",
    "if train_cols == test_cols:\n",
    "    print(\"Feature columns match between train and test!\")\n",
    "else:\n",
    "    print(\"Column differences:\")\n",
    "    print(f\"  In train only: {train_cols - test_cols}\")\n",
    "    print(f\"  In test only: {test_cols - train_cols}\")\n",
    "\n",
    "# Check for missing values in test\n",
    "missing_test = test_features.isnull().sum()\n",
    "if missing_test.sum() > 0:\n",
    "    print(\"\\nMissing values in test:\")\n",
    "    print(missing_test[missing_test > 0])\n",
    "else:\n",
    "    print(\"\\nNo missing values in test features!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "save-test-features",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test features saved to test_features.parquet\n"
     ]
    }
   ],
   "source": [
    "# Save test features\n",
    "test_features.to_parquet('test_features.parquet', index=False)\n",
    "print(\"Test features saved to test_features.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "day1-summary",
   "metadata": {},
   "source": [
    "## Day 1 Summary\n",
    "\n",
    "### Completed:\n",
    "- Loaded train (17.5M events) and test (4.4M events) data\n",
    "- Created user-level churn labels (22% churn rate)\n",
    "- Built feature engineering function with 35+ features\n",
    "- Created training features for 19,140 users\n",
    "- Created test features for 2,904 users\n",
    "- Saved features as parquet files\n",
    "\n",
    "### Feature Categories:\n",
    "| Category | Features |\n",
    "|----------|----------|\n",
    "| Engagement | total_events, total_songs, total_sessions, avg/max/std_session_length |\n",
    "| Page counts | page_nextsong, page_thumbs_up/down, page_downgrade, etc. |\n",
    "| Behavioral ratios | thumbs_up/down_ratio, playlist_add_ratio, error_rate, ad_ratio |\n",
    "| Temporal | days_active, days_since_registration, events_per_day, activity_trend |\n",
    "| Subscription | is_paid, level_changes, has_downgrade, has_upgrade, paid_ratio |\n",
    "| Content | unique_songs/artists, avg_song_length, total_listen_time, song_repeat_ratio |\n",
    "| Demographics | gender, state |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "final-summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "DAY 1 COMPLETE\n",
      "==================================================\n",
      "\n",
      "Training set: 19,140 users, 43 features\n",
      "Test set: 2,904 users, 43 features\n",
      "\n",
      "Churn rate: 22.31%\n",
      "\n",
      "Files created:\n",
      "  - train_features.parquet\n",
      "  - test_features.parquet\n"
     ]
    }
   ],
   "source": [
    "# Final summary\n",
    "print(\"=\"*50)\n",
    "print(\"DAY 1 COMPLETE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nTraining set: {train_features.shape[0]:,} users, {train_features.shape[1]-2} features\")\n",
    "print(f\"Test set: {test_features.shape[0]:,} users, {test_features.shape[1]-1} features\")\n",
    "print(f\"\\nChurn rate: {train_features['churn'].mean():.2%}\")\n",
    "print(f\"\\nFiles created:\")\n",
    "print(\"  - train_features.parquet\")\n",
    "print(\"  - test_features.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
